{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b771286e",
   "metadata": {},
   "source": [
    "# SAMS Student Data Analysis\n",
    "\n",
    "This notebook processes and analyzes student application and enrollment data across multiple education modules (ITI, Diploma, HSS, DEG) to create unique student identifiers and generate summary statistics.\n",
    "\n",
    "**Main Steps:**\n",
    "1. Load data from SQLite database and Parquet files\n",
    "2. Clean and standardize student records\n",
    "3. Decrypt and validate roll numbers\n",
    "4. Generate unique student keys for identity matching\n",
    "5. Merge applications with enrollment data\n",
    "6. Analyze Aadhaar coverage and generate summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a85a57",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Import core libraries and project helpers used across the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "258cfb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-13 12:03:54.674\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msams.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mPROJ_ROOT path is: C:\\Users\\Admin\\Documents\\GitHub\\sams\u001b[0m\n",
      "\u001b[32m2025-11-13 12:03:54.711\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36msams.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m83\u001b[0m - \u001b[33m\u001b[1mGoogle MAPS API key not found, using Nominatim geocoder\u001b[0m\n",
      "\u001b[32m2025-11-13 12:03:54.711\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msams.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m92\u001b[0m - \u001b[1mLoaded 0 geocodes from cache\u001b[0m\n",
      "\u001b[32m2025-11-13 12:03:54.711\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36msams.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m83\u001b[0m - \u001b[33m\u001b[1mGoogle MAPS API key not found, using Nominatim geocoder\u001b[0m\n",
      "\u001b[32m2025-11-13 12:03:54.711\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msams.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m92\u001b[0m - \u001b[1mLoaded 0 geocodes from cache\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from sams.utils import load_data\n",
    "from sams.config import datasets\n",
    "import gc\n",
    "import polars as pl\n",
    "import duckdb\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530e77e8",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "\n",
    "### 2.1 Connect to SQLite Database\n",
    "Check connection and list available tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "345f130a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables: ['students', 'institutes', 'results']\n"
     ]
    }
   ],
   "source": [
    "# Use the path from datasets metadata \n",
    "db_path = datasets[\"sams\"][\"path\"]\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "print(\"Tables:\", [t[0] for t in tables])\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ec6f81",
   "metadata": {},
   "source": [
    "### 2.2 Extract ITI & Diploma Data from SQLite\n",
    "Define a helper that explodes `mark_data` JSON using DuckDB and returns tidy rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a893009a",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "\n",
    "### 2.1 Connect to SQLite Database\n",
    "\n",
    "Verify database connection and check available tables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dab65d",
   "metadata": {},
   "source": [
    "HA=ad to use this method because for iti amrks and diploma amrks parquet file doesn't include the stdunet name and to expand them varefully, i made this approach since we have non reliable aadhr numbe rfor some year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bedb3fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = datasets[\"sams\"][\"path\"]\n",
    "\n",
    "# Load ITI + Diploma\n",
    "conn = sqlite3.connect(db_path)\n",
    "query = \"\"\"\n",
    "SELECT academic_year, aadhar_no, student_name, dob,\n",
    "       admission_status, mark_data, module\n",
    "FROM students\n",
    "WHERE module IN ('ITI', 'Diploma');\n",
    "\"\"\"\n",
    "students_df = pd.read_sql_query(query, conn)\n",
    "conn.close()\n",
    "\n",
    "# Extract needed keys (first entry only)\n",
    "keep_keys = [\"YearofPassing\", \"RollNo\", \"ExaminationType\",\n",
    "             \"HighestQualificationExamBoard\", \"ExamName\"]\n",
    "\n",
    "def extract_mark(row):\n",
    "    md = row[\"mark_data\"]\n",
    "    try:\n",
    "        md = json.loads(md) if isinstance(md, str) else md\n",
    "        if isinstance(md, dict): md = [md]\n",
    "    except:\n",
    "        md = []\n",
    "    rec = md[0] if isinstance(md, list) and md else {}\n",
    "    return {k: rec.get(k) for k in keep_keys}\n",
    "\n",
    "mark_df = pd.json_normalize(students_df.apply(extract_mark, axis=1))\n",
    "\n",
    "# Merge + rename\n",
    "df = pd.concat([students_df.drop(columns=[\"mark_data\"]),\n",
    "                mark_df.rename(columns={\n",
    "                    \"YearofPassing\": \"passing_year\",\n",
    "                    \"RollNo\": \"roll_no\",\n",
    "                    \"ExaminationType\": \"exam_type\",\n",
    "                    \"HighestQualificationExamBoard\": \"exam_board\",\n",
    "                    \"ExamName\": \"exam_name\"\n",
    "                })],\n",
    "               axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e084dac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ITI (all rows)\n",
    "iti_enrollments = df[df[\"module\"] == \"ITI\"].reset_index(drop=True)\n",
    "\n",
    "# Diploma (ONLY exam_name = \"10th\")\n",
    "diploma_enrollments = (\n",
    "    df[(df[\"module\"] == \"Diploma\") &\n",
    "       (df[\"exam_name\"].str.lower() == \"10th\")]\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42fb6ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the dob in iti_enrollments to YYYY-MM-DD format\n",
    "iti_enrollments['dob'] = pd.to_datetime(iti_enrollments['dob'], format=\"%d-%b-%Y\", errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
    "diploma_enrollments['dob'] = pd.to_datetime(diploma_enrollments['dob'], format=\"%d-%b-%Y\", errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49d2037",
   "metadata": {},
   "outputs": [],
   "source": [
    "iti_marks = load_data(datasets['iti_marks'])\n",
    "diploma_marks = load_data(datasets['diploma_marks'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad1e84c",
   "metadata": {},
   "source": [
    "### 2.3 Load Application Data from Parquet Files\n",
    "Load interim Parquet files for ITI and Diploma applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fd8f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df in parquet format\n",
    "DATA_DIR = Path(\"C:/Users/Admin/Documents/GitHub/sams/data\")\n",
    "RAW_DATA_DIR = DATA_DIR / \"interim\"\n",
    "iti_applications = pd.read_parquet(RAW_DATA_DIR / \"iti_applications.pq\")\n",
    "diploma_applications = pd.read_parquet(RAW_DATA_DIR / \"diploma_applications.pq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bef743",
   "metadata": {},
   "source": [
    "### 2.4 Load Enrollment Data for All Modules\n",
    "Load enrollment/application datasets used later for keys, merges, and summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ac21297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-13 12:06:41.081\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msams.utils\u001b[0m:\u001b[36mload_data\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mLoading data from C:\\Users\\Admin\\Documents\\GitHub\\sams\\data\\interim\\hss_enrollments.pq\u001b[0m\n",
      "\u001b[32m2025-11-13 12:07:27.369\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msams.utils\u001b[0m:\u001b[36mload_data\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mLoading data from C:\\Users\\Admin\\Documents\\GitHub\\sams\\data\\interim\\deg_enrollments.pq\u001b[0m\n",
      "\u001b[32m2025-11-13 12:07:27.369\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msams.utils\u001b[0m:\u001b[36mload_data\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mLoading data from C:\\Users\\Admin\\Documents\\GitHub\\sams\\data\\interim\\deg_enrollments.pq\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "hss_enrollments = load_data(datasets[\"hss_enrollments\"])\n",
    "deg_enrollments = load_data(datasets[\"deg_enrollments\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b0b4288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-13 12:08:06.520\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msams.utils\u001b[0m:\u001b[36mload_data\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mLoading data from C:\\Users\\Admin\\Documents\\GitHub\\sams\\data\\interim\\deg_applications.pq\u001b[0m\n",
      "\u001b[32m2025-11-13 12:08:19.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msams.utils\u001b[0m:\u001b[36mload_data\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mLoading data from C:\\Users\\Admin\\Documents\\GitHub\\sams\\data\\interim\\hss_applications.pq\u001b[0m\n",
      "\u001b[32m2025-11-13 12:08:19.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msams.utils\u001b[0m:\u001b[36mload_data\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mLoading data from C:\\Users\\Admin\\Documents\\GitHub\\sams\\data\\interim\\hss_applications.pq\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "deg_applications = load_data(datasets[\"deg_applications\"])\n",
    "hss_applications = load_data(datasets[\"hss_applications\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b390531f",
   "metadata": {},
   "source": [
    "### 2.5 Extract CHSE and BSE Results Data\n",
    "\n",
    "Extract board exam results for CHSE (Higher Secondary) and BSE (Secondary) from the results table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c34816c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = datasets[\"sams\"][\"path\"]\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    academic_year,\n",
    "    student_name,\n",
    "    dob,\n",
    "    module,\n",
    "    CASE \n",
    "        WHEN module = 'CHSE' THEN 'CHSE, Odisha'\n",
    "        WHEN module = 'BSE' THEN 'BSE, Odisha'\n",
    "        ELSE NULL\n",
    "    END AS exam_board,\n",
    "    academic_year AS passing_year,    \n",
    "    roll_no,\n",
    "    NULL AS roll_no_decrypted,\n",
    "    exam_type\n",
    "    \n",
    "FROM results\n",
    "WHERE module IN ('CHSE', 'BSE');\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "conn.close()\n",
    "\n",
    "# Split into CHSE and BSE datasets\n",
    "chse_df = df[df[\"module\"] == \"CHSE\"].reset_index(drop=True)\n",
    "bse_df  = df[df[\"module\"] == \"BSE\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d34c8e2",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning and Standardization\n",
    "\n",
    "### 3.1 Standardize Date Formats and Exam Types\n",
    "\n",
    "Normalize date formats for BSE and standardize exam type terminology for CHSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c623411",
   "metadata": {},
   "outputs": [],
   "source": [
    "bse_df[\"dob\"] = pd.to_datetime(bse_df[\"dob\"], format=\"%d-%b-%Y\", errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
    "chse_df['exam_type'] = chse_df['exam_type'].replace({'REGULAR': 'annual'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5b073a",
   "metadata": {},
   "source": [
    "## 4. Roll Number Decryption and Validation\n",
    "\n",
    "### 4.1 Define Decryption Function\n",
    "\n",
    "Decrypt AES-encrypted roll numbers from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90592cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for decryption\n",
    "from base64 import b64decode\n",
    "from Crypto.Cipher import AES\n",
    "\n",
    "def decrypt_roll(enc_text: str,\n",
    "                 key: bytes = b\"y6idXfCVRG5t2dkeBnmHy9jLu6TEn5Du\",\n",
    "                 enforce_min_length: bool = False,\n",
    "                 min_length: int = None) -> str:\n",
    "    try:\n",
    "        if not enc_text or not isinstance(enc_text, str):\n",
    "            return \"NA\"\n",
    "\n",
    "        raw = b64decode(enc_text)\n",
    "        cipher = AES.new(key, AES.MODE_ECB)\n",
    "        decrypted = cipher.decrypt(raw)\n",
    "\n",
    "        pad_len = decrypted[-1]\n",
    "        if pad_len < 1 or pad_len > 16:\n",
    "            return \"NA\"\n",
    "        decrypted = decrypted[:-pad_len]\n",
    "\n",
    "        roll_no = decrypted.decode(\"utf-8\").strip()\n",
    "        return roll_no\n",
    "    except Exception:\n",
    "        return \"NA\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7610ab13",
   "metadata": {},
   "source": [
    "### 4.2 Decrypt and Validate Roll Numbers\n",
    "\n",
    "Validate decrypted roll numbers based on board-specific length requirements:\n",
    "- BSE Odisha: 9 characters\n",
    "- CHSE Odisha: 8 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02b2c245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_roll_numbers_len_format(df: pd.DataFrame, roll_col: str = 'roll_no') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Decrypt roll numbers and validate only by length rule:\n",
    "    - BSE Odisha: length must be 9\n",
    "    - CHSE Odisha: length must be 8\n",
    "    - Other boards: keep decrypted roll as-is\n",
    "    \"\"\"\n",
    "\n",
    "    # Decrypt roll numbers\n",
    "    df['roll_no_decrypted'] = df[roll_col].map(decrypt_roll)\n",
    "\n",
    "    # Identify Odisha boards \n",
    "    board_col = df['exam_board'].fillna(\"NA\").str.upper()\n",
    "    # Put the condition to pass these input values of board name        \n",
    "    mask_bse = (board_col.str.contains(r'\\bBOARD OF SECONDARY EDUCATION,\\s*ODISHA\\b', regex=True)  \n",
    "                | (board_col.str.contains(r'\\bBSE\\b(?! MADHYAMA).*ODISHA\\b', regex=True) & ~board_col.str.contains(r'\\bICSE\\b|\\bCBSE\\b', regex=True)))\n",
    "    \n",
    "    mask_chse = (board_col.str.contains(r'\\bCOUNCIL OF HIGHER SECONDARY EDUCATION,\\s*ODISHA\\b', regex=True) \n",
    "                 | board_col.str.contains(r'\\bCHSE\\b.*ODISHA\\b', regex=True))\n",
    "\n",
    "    # Apply validation\n",
    "    if mask_bse.any():\n",
    "        rolls_bse = df.loc[mask_bse & df['roll_no_decrypted'].notna(), 'roll_no_decrypted'].astype(str)\n",
    "        valid_bse = rolls_bse.str.len() == 9\n",
    "        df.loc[mask_bse & ~valid_bse, 'roll_no_decrypted'] = 'NA'\n",
    "\n",
    "    if mask_chse.any():\n",
    "        rolls_chse = df.loc[mask_chse & df['roll_no_decrypted'].notna(), 'roll_no_decrypted'].astype(str)\n",
    "        valid_chse = rolls_chse.str.len() == 8\n",
    "        df.loc[mask_chse & ~valid_chse, 'roll_no_decrypted'] = 'NA'\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c3031a",
   "metadata": {},
   "source": [
    "### 3.2 Prepare HSS and DEG Data\n",
    "\n",
    "Select relevant columns and rename to match standard schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86c3031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to keep, then drop everything else and rename finally\n",
    "keep_cols = [\n",
    "    'barcode', 'aadhar_no', 'academic_year', 'module', 'student_name',\n",
    "    'dob', 'examination_board_of_the_highest_qualification','examination_type', 'year_of_passing' , 'roll_no'\n",
    "]\n",
    "hss_enrollments = hss_enrollments[keep_cols].copy()\n",
    "deg_enrollments = deg_enrollments[keep_cols].copy()\n",
    "rename_map = {\n",
    "    'examination_board_of_the_highest_qualification': 'exam_board',\n",
    "    'examination_type': 'exam_type',\n",
    "    'year_of_passing': 'passing_year'\n",
    "}\n",
    "hss_enroll = hss_enrollments.rename(columns=rename_map)\n",
    "deg_enroll = deg_enrollments.rename(columns=rename_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6291337c",
   "metadata": {},
   "source": [
    "### 4.3 Apply Decryption to All Datasets\n",
    "\n",
    "Process roll numbers for all modules (ITI, Diploma, HSS, DEG, BSE, CHSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58446d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "iti_df = process_roll_numbers_len_format(iti_enrollments)\n",
    "diploma_df = process_roll_numbers_len_format(diploma_enrollments)\n",
    "hss_df = process_roll_numbers_len_format(hss_enroll)\n",
    "deg_df = process_roll_numbers_len_format(deg_enroll)\n",
    "# bse_df = process_roll_numbers_len_format(bse_df)\n",
    "# chse_df = process_roll_numbers_len_format(chse_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2458b6f8",
   "metadata": {},
   "source": [
    "## 5. Generate Student Keys\n",
    "\n",
    "### 5.1 Define Student Key Generation Functions\n",
    "\n",
    "Create unique student identifiers using name, roll number, DOB, passing year, exam board, and exam type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1832a170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_part(s: pd.Series, *, na_label=\"NA\", missing_label=\"MISSING\", lower=False) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Encode parts of a student key by handling missing/NA values consistently.\n",
    "    \"\"\"\n",
    "    is_nan = s.isna()\n",
    "    t = s.astype(str).str.strip()\n",
    "    t = t.str.strip('\"').str.strip(\"'\")   # remove quotes if present\n",
    "\n",
    "    out = t.copy()\n",
    "\n",
    "    # Replace explicit NA and missing values\n",
    "    out = out.mask(t.eq(\"NA\"), na_label)\n",
    "    out = out.mask(t.eq(\"\") | is_nan, missing_label)\n",
    "\n",
    "    # Normalize casing if requested\n",
    "    if lower:\n",
    "        out = out.where(out.isin([na_label, missing_label]), out.str.lower().str.strip())\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77118bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_student_key_df(df, module_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean key columns in-place, then generate a student_key\n",
    "    and print diagnostics about duplicates.\n",
    "    \"\"\"\n",
    "    new_df = df.copy()\n",
    "\n",
    "    key_vars = [\"passing_year\", \"dob\",\n",
    "                \"roll_no_decrypted\", \"exam_board\", \"exam_type\"]\n",
    "\n",
    "    # Normalize and ensure all key parts are strings\n",
    "    for col in key_vars + [\"student_name\"]:\n",
    "        new_df[col] = new_df[col].astype(str).fillna(\"\").str.strip().str.lower()\n",
    "\n",
    "    # Construct student key safely\n",
    "    new_df[\"student_key\"] = (\n",
    "        new_df[\"student_name\"] + \"_\" +\n",
    "        new_df[\"roll_no_decrypted\"] + \"_\" +\n",
    "        new_df[\"dob\"] + \"_\" +\n",
    "        new_df[\"passing_year\"] + \"_\" +\n",
    "        new_df[\"exam_board\"] + \"_\" +\n",
    "        new_df[\"exam_type\"]\n",
    "    )\n",
    "\n",
    "    # Diagnostics\n",
    "    total_records = len(new_df)\n",
    "    unique_aadhar = new_df[\"aadhar_no\"].nunique(dropna=True)\n",
    "    unique_keys = new_df[\"student_key\"].nunique()\n",
    "\n",
    "    # Problematic duplicates = same key linked to multiple Aadhaar numbers\n",
    "    dup_check = (\n",
    "        new_df.groupby(\"student_key\")[\"aadhar_no\"]\n",
    "        .nunique(dropna=True)\n",
    "        .reset_index(name=\"unique_aadhar_count\")\n",
    "    )\n",
    "    problematic_keys = dup_check[dup_check[\"unique_aadhar_count\"] > 1][\"student_key\"]\n",
    "    duplicate_keys_count = len(problematic_keys)\n",
    "\n",
    "    print(f\"\\n[{module_name}]\")\n",
    "    print(\"Total student records:\", total_records)\n",
    "    print(\"Unique student keys generated:\", unique_keys)\n",
    "    print(\"Duplicate student keys:\", duplicate_keys_count)\n",
    "    print(\"Unique Aadhar numbers:\", unique_aadhar)\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80c9825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_student_key_four_var(df: pd.DataFrame, module_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate a standardized 4-variable student key (`student_key_4_var`) \n",
    "    for identity matching across datasets.\n",
    "\n",
    "    The key is built using module-specific rules:\n",
    "    - CHSE (Higher Secondary): roll_no_decrypted + passing_year + exam_board + exam_type\n",
    "    - BSE  (Secondary):        roll_no_decrypted + dob + passing_year + exam_board\n",
    "    - DEG  (Degree):           roll_no_decrypted + passing_year + exam_board + exam_type\n",
    "    - HSS  (Higher Secondary): roll_no_decrypted + dob + passing_year + exam_board\n",
    "\n",
    "    All fields are normalized (lowercase, stripped) before concatenation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame containing student records.\n",
    "    module_name : str\n",
    "        Module name (\"CHSE\", \"BSE\", \"DEG\", or \"HSS\")\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with a new column `student_key_4_var`\n",
    "    \"\"\"\n",
    "\n",
    "    new_df = df.copy()\n",
    "    module = module_name.upper()\n",
    "\n",
    "    # Select key components based on module\n",
    "    if module in [\"CHSE\", \"DEG\"]:\n",
    "        key_parts = [\"roll_no_decrypted\", \"passing_year\", \"exam_board\", \"exam_type\"]\n",
    "    elif module in [\"BSE\", \"HSS\"]:\n",
    "        key_parts = [\"roll_no_decrypted\", \"dob\", \"passing_year\", \"exam_board\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid module '{module_name}'. Use 'CHSE', 'BSE', 'DEG', or 'HSS'.\")\n",
    "\n",
    "    # Normalize fields\n",
    "    for col in key_parts:\n",
    "        new_df[col] = (\n",
    "            new_df[col].astype(str).fillna(\"\").str.strip().str.lower()\n",
    "            if col in new_df.columns else \"\"\n",
    "        )\n",
    "\n",
    "    # Create composite key\n",
    "    new_df[\"student_key_4_var\"] = new_df[key_parts].agg(\"_\".join, axis=1)\n",
    "\n",
    "    # Summary\n",
    "    print(f\"\\n[{module}] Student Key (4-var) Summary\")\n",
    "    print(\"Total records:\", len(new_df))\n",
    "    print(\"Unique keys:\", new_df[\"student_key_4_var\"].nunique())\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b48da6",
   "metadata": {},
   "source": [
    "### 5.2 Generate Keys for All Modules\n",
    "\n",
    "Create student keys for ITI, Diploma, HSS, and DEG with diagnostic output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f6838c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ITI]\n",
      "Total student records: 559575\n",
      "Unique student keys generated: 524796\n",
      "Duplicate student keys: 1807\n",
      "Unique Aadhar numbers: 518024\n",
      "\n",
      "[Diploma]\n",
      "Total student records: 445850\n",
      "Unique student keys generated: 414078\n",
      "Duplicate student keys: 4285\n",
      "Unique Aadhar numbers: 392904\n",
      "\n",
      "[Diploma]\n",
      "Total student records: 445850\n",
      "Unique student keys generated: 414078\n",
      "Duplicate student keys: 4285\n",
      "Unique Aadhar numbers: 392904\n",
      "\n",
      "[DEG]\n",
      "Total student records: 2054491\n",
      "Unique student keys generated: 1634565\n",
      "Duplicate student keys: 1917\n",
      "Unique Aadhar numbers: 1506304\n",
      "\n",
      "[DEG]\n",
      "Total student records: 2054491\n",
      "Unique student keys generated: 1634565\n",
      "Duplicate student keys: 1917\n",
      "Unique Aadhar numbers: 1506304\n"
     ]
    }
   ],
   "source": [
    "iti_key_df = generate_student_key_df(iti_df, \"ITI\")\n",
    "diploma_key_df = generate_student_key_df(diploma_df, \"Diploma\")\n",
    "# hss_key_df = generate_student_key_df(hss_df, \"HSS\")\n",
    "deg_key_df = generate_student_key_df(deg_df, \"DEG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c30a931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HSS]\n",
      "Total student records: 3453401\n",
      "Unique student keys generated: 2961052\n",
      "Duplicate student keys: 5011\n",
      "Unique Aadhar numbers: 2650769\n"
     ]
    }
   ],
   "source": [
    "hss_key_df = generate_student_key_df(hss_df, \"HSS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c0db7c",
   "metadata": {},
   "source": [
    "## 6. Merge Applications with Enrollment Data\n",
    "\n",
    "Link application records with student identity information from enrollment data using barcode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35fd38f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_enrollment_applications(enroll_df, app_df):\n",
    "    \"\"\"\n",
    "    Merge application rows with student identity columns using barcode.\n",
    "    Keeps all application rows and adds student info from enrollment table.\n",
    "    \"\"\"\n",
    "\n",
    "    # Columns we want from enrollment (as you listed)\n",
    "    enroll_cols = [\n",
    "        \"barcode\", \"student_name\", \"aadhar_no\", \"dob\", \"module\",\n",
    "        \"academic_year\", \"exam_board\", \"exam_type\", \"passing_year\",\n",
    "        \"roll_no\", \"roll_no_decrypted\", \"student_key\"\n",
    "    ]\n",
    "    \n",
    "    enroll_reduced = enroll_df[enroll_cols].copy()\n",
    "\n",
    "    # Only barcode from applications\n",
    "    app_reduced = app_df[[\"barcode\"]].copy()\n",
    "\n",
    "    # Merge keeping all application rows\n",
    "    merged = app_reduced.merge(enroll_reduced, on=\"barcode\", how=\"left\")\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5772f334",
   "metadata": {},
   "outputs": [],
   "source": [
    "hss_df2 = merge_enrollment_applications(hss_key_df, hss_applications)\n",
    "deg_df2 = merge_enrollment_applications(deg_key_df, deg_applications)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed8604f",
   "metadata": {},
   "source": [
    "## 7. Summary Statistics and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd71e002",
   "metadata": {},
   "source": [
    "### 7.1 Repeated Aadhaar Analysis\n",
    "\n",
    "Check for Aadhaar numbers repeated within a year (above a chosen threshold, e.g., > 10) to flag potential data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0565601b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ITI ENROLLMENT: Repeated Aadhaar by Year ---\n",
      " academic_year                                    aadhar_no  count\n",
      "          2017 47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=   3181\n",
      "          2018 UIC27nlODwzAwV13RAZD1vk8kiSxo2GLRDviArS4Ktg=     10\n",
      " academic_year                                    aadhar_no  count\n",
      "          2017 47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=   3181\n",
      "          2018 UIC27nlODwzAwV13RAZD1vk8kiSxo2GLRDviArS4Ktg=     10\n"
     ]
    }
   ],
   "source": [
    "# Simple repeat Aadhaar check by academic year\n",
    "\n",
    "def repeated_aadhaar_summary(df, module_name):\n",
    "    print(f\"\\n--- {module_name.upper()}: Repeated Aadhaar by Year ---\")\n",
    "    repeat_summary = (\n",
    "        df.groupby(['academic_year', 'aadhar_no'])\n",
    "          .size()\n",
    "          .reset_index(name='count')\n",
    "          .query('count > 5')  # Only keep Aadhaar appearing more than five times\n",
    "          .sort_values(['academic_year', 'count'], ascending=[True, False])\n",
    "    )\n",
    "    print(repeat_summary.to_string(index=False))\n",
    "\n",
    "# Run for ITI and Diploma\n",
    "repeated_aadhaar_summary(iti_enrollments, \"ITI ENROLLMENT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bbcd4d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DIPLOMA ENROLLMENT: Repeated Aadhaar by Year ---\n",
      " academic_year                                    aadhar_no  count\n",
      "          2018 47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=  10118\n",
      "          2019 47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=  13183\n",
      " academic_year                                    aadhar_no  count\n",
      "          2018 47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=  10118\n",
      "          2019 47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=  13183\n"
     ]
    }
   ],
   "source": [
    "repeated_aadhaar_summary(diploma_enrollments, \"Diploma ENROLLMENT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e3d557bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITI missing (enrollments, null + repeated hashes):\n",
      " academic_year  iti_missing_aadhar\n",
      "          2017             10949.0\n",
      "          2018              8840.0\n",
      "          2019                 0.0\n",
      "          2020                 4.0\n",
      "          2021                 0.0\n",
      "          2022                 0.0\n",
      "          2023                 0.0\n",
      "\n",
      "Diploma missing (enrollments, null + repeated hashes):\n",
      " academic_year  diploma_missing_aadhar\n",
      "          2018                 12079.0\n",
      "          2019                 13329.0\n",
      "          2020                   107.0\n",
      "          2021                     0.0\n",
      "          2022                     0.0\n",
      "          2023                     0.0\n",
      "          2024                     0.0\n"
     ]
    }
   ],
   "source": [
    "# Missing Aadhaar by academic_year from enrollments\n",
    "# Treat repeated non-null Aadhaar values (hashed/placeholder duplicates) as missing as well\n",
    "\n",
    "def missing_from_enrollments(df: pd.DataFrame, prefix: str) -> pd.DataFrame:\n",
    "    if 'aadhar_no' not in df.columns or 'academic_year' not in df.columns:\n",
    "        # Return empty/zero table if required columns are missing\n",
    "        return pd.DataFrame({'academic_year': [], f'{prefix}_missing_aadhar': []})\n",
    "\n",
    "    g = df[['academic_year', 'aadhar_no']].copy()\n",
    "\n",
    "    # Base missing = explicit nulls\n",
    "    missing_null = (\n",
    "        g.groupby('academic_year', dropna=False)['aadhar_no']\n",
    "         .apply(lambda s: s.isna().sum())\n",
    "         .rename('missing_null')\n",
    "    )\n",
    "\n",
    "    # Hash/placeholder duplicates: non-null Aadhaar values that repeat > 1 in the year\n",
    "    non_null = g[g['aadhar_no'].notna()].copy()\n",
    "    if not non_null.empty:\n",
    "        dup_counts = (\n",
    "            non_null.groupby(['academic_year', 'aadhar_no'])\n",
    "                    .size()\n",
    "                    .reset_index(name='cnt')\n",
    "        )\n",
    "        hashed_rows_by_year = (\n",
    "            dup_counts[dup_counts['cnt'] > 1]\n",
    "            .groupby('academic_year')['cnt']\n",
    "            .sum()\n",
    "            .rename('hashed_rows')\n",
    "        )\n",
    "    else:\n",
    "        hashed_rows_by_year = pd.Series(dtype='int64', name='hashed_rows')\n",
    "\n",
    "    # Combine null-missing and hashed duplicate rows as total missing\n",
    "    out = missing_null.to_frame().join(hashed_rows_by_year, how='left').fillna(0)\n",
    "    out['missing_aadhar'] = out['missing_null'] + out['hashed_rows']\n",
    "\n",
    "    res = (\n",
    "        out[['missing_aadhar']]\n",
    "        .reset_index()\n",
    "        .rename(columns={'missing_aadhar': f'{prefix}_missing_aadhar'})\n",
    "    )\n",
    "    return res\n",
    "\n",
    "iti_missing = missing_from_enrollments(iti_key_df, 'iti')\n",
    "dip_missing = missing_from_enrollments(diploma_key_df, 'diploma')\n",
    "\n",
    "print('ITI missing (null + repeated hashes):')\n",
    "print(iti_missing.head(7).to_string(index=False))\n",
    "print('\\nDiploma missing (null + repeated hashes):')\n",
    "print(dip_missing.head(7).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62f3b4f",
   "metadata": {},
   "source": [
    "### 7.3 Aadhaar Coverage Summary\n",
    "\n",
    "Calculate, by academic year:\n",
    "- Total applications\n",
    "- Unique Aadhaar count\n",
    "- Non-unique (duplicate) Aadhaar IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fd939089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.x Unreliable Aadhaar detector (per-year)\n",
    "# Treats nulls as missing and flags heavy repeated placeholders per year.\n",
    "# Also allows explicit known placeholder hashes.\n",
    "\n",
    "KNOWN_AADHAAR_PLACEHOLDERS = {\n",
    "    # SHA-1 of empty string (Base64) often used as placeholder\n",
    "    '47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=',\n",
    "}\n",
    "\n",
    "\n",
    "def build_unreliable_aadhaar_index(df: pd.DataFrame,\n",
    "                                    per_year_threshold: int = 100,\n",
    "                                    known_placeholders: set[str] | None = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return a DataFrame of (academic_year, aadhar_no, unreliable=True) pairs.\n",
    "    - Unreliable if value appears >= per_year_threshold within that year\n",
    "    - Unreliable if value is in known_placeholders (for any year it appears)\n",
    "    \"\"\"\n",
    "    known_placeholders = known_placeholders or set()\n",
    "\n",
    "    # Per-year counts (ignore nulls)\n",
    "    counts = (\n",
    "        df.dropna(subset=['aadhar_no'])\n",
    "          .groupby(['academic_year', 'aadhar_no'])\n",
    "          .size().reset_index(name='cnt')\n",
    "    )\n",
    "\n",
    "    # Heavy repeats within the year\n",
    "    heavy = counts[counts['cnt'] >= per_year_threshold][['academic_year', 'aadhar_no']]\n",
    "\n",
    "    # Known placeholders wherever they appear\n",
    "    known = counts[counts['aadhar_no'].isin(known_placeholders)][['academic_year', 'aadhar_no']]\n",
    "\n",
    "    out = pd.concat([heavy, known], axis=0).drop_duplicates()\n",
    "    if out.empty:\n",
    "        out = pd.DataFrame(columns=['academic_year', 'aadhar_no'])\n",
    "    out['unreliable'] = True\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "db55cbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_9772\\1158881523.py:19: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  g['unreliable'] = g['unreliable'].fillna(False)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_9772\\1158881523.py:19: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  g['unreliable'] = g['unreliable'].fillna(False)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_9772\\1158881523.py:19: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  g['unreliable'] = g['unreliable'].fillna(False)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "year",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "iti_unique_aadhar",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "iti_supposed_missing",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "iti_non_unique_aadhar",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "diploma_unique_aadhar",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "diploma_supposed_missing",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "diploma_non_unique_aadhar",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "ed9efa31-2506-47a0-a5d2-2d5d04341f52",
       "rows": [
        [
         "0",
         "2017",
         "27336",
         "3181",
         "3631",
         null,
         null,
         null
        ],
        [
         "1",
         "2018",
         "70612",
         "1",
         "4242",
         "49933.0",
         "10118.0",
         "975.0"
        ],
        [
         "2",
         "2019",
         "64148",
         "0",
         "0",
         "41589.0",
         "13183.0",
         "73.0"
        ],
        [
         "3",
         "2020",
         "67411",
         "4",
         "0",
         "52958.0",
         "5.0",
         "51.0"
        ],
        [
         "4",
         "2021",
         "68000",
         "0",
         "0",
         "54979.0",
         "0.0",
         "0.0"
        ],
        [
         "5",
         "2022",
         "74104",
         "0",
         "0",
         "71768.0",
         "0.0",
         "0.0"
        ],
        [
         "6",
         "2023",
         "92085",
         "0",
         "0",
         "73513.0",
         "0.0",
         "0.0"
        ],
        [
         "7",
         "2024",
         "83958",
         "0",
         "0",
         "76694.0",
         "0.0",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>iti_unique_aadhar</th>\n",
       "      <th>iti_supposed_missing</th>\n",
       "      <th>iti_non_unique_aadhar</th>\n",
       "      <th>diploma_unique_aadhar</th>\n",
       "      <th>diploma_supposed_missing</th>\n",
       "      <th>diploma_non_unique_aadhar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>27336</td>\n",
       "      <td>3181</td>\n",
       "      <td>3631</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>70612</td>\n",
       "      <td>1</td>\n",
       "      <td>4242</td>\n",
       "      <td>49933.0</td>\n",
       "      <td>10118.0</td>\n",
       "      <td>975.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019</td>\n",
       "      <td>64148</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41589.0</td>\n",
       "      <td>13183.0</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>67411</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>52958.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021</td>\n",
       "      <td>68000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54979.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022</td>\n",
       "      <td>74104</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71768.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023</td>\n",
       "      <td>92085</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73513.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024</td>\n",
       "      <td>83958</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>76694.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  iti_unique_aadhar  iti_supposed_missing  iti_non_unique_aadhar  \\\n",
       "0  2017              27336                  3181                   3631   \n",
       "1  2018              70612                     1                   4242   \n",
       "2  2019              64148                     0                      0   \n",
       "3  2020              67411                     4                      0   \n",
       "4  2021              68000                     0                      0   \n",
       "5  2022              74104                     0                      0   \n",
       "6  2023              92085                     0                      0   \n",
       "7  2024              83958                     0                      0   \n",
       "\n",
       "   diploma_unique_aadhar  diploma_supposed_missing  diploma_non_unique_aadhar  \n",
       "0                    NaN                       NaN                        NaN  \n",
       "1                49933.0                   10118.0                      975.0  \n",
       "2                41589.0                   13183.0                       73.0  \n",
       "3                52958.0                       5.0                       51.0  \n",
       "4                54979.0                       0.0                        0.0  \n",
       "5                71768.0                       0.0                        0.0  \n",
       "6                73513.0                       0.0                        0.0  \n",
       "7                76694.0                       0.0                        0.0  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7.x Clean Aadhaar summary (per-year) using unreliable index\n",
    "# Computes per-year metrics after excluding nulls and unreliable (hashed/heavy-repeat) values.\n",
    "\n",
    "\n",
    "def summarize_clean_aadhaar(df: pd.DataFrame,\n",
    "                            prefix: str,\n",
    "                            unreliable_idx: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns per-year metrics:\n",
    "    - {prefix}_unique_aadhar: distinct Aadhaar among reliable rows\n",
    "    - {prefix}_supposed_missing: rows with null or unreliable Aadhaar\n",
    "    - {prefix}_non_unique_aadhar: count of Aadhaar IDs still appearing >1 among reliable rows\n",
    "    \"\"\"\n",
    "    g = df[['academic_year', 'aadhar_no']].copy()\n",
    "\n",
    "    # Join unreliable pairs on (year, aadhar)\n",
    "    if unreliable_idx is not None and not unreliable_idx.empty:\n",
    "        g = g.merge(unreliable_idx, on=['academic_year', 'aadhar_no'], how='left')\n",
    "        g['unreliable'] = g['unreliable'].fillna(False)\n",
    "    else:\n",
    "        g['unreliable'] = False\n",
    "\n",
    "    g['is_null'] = g['aadhar_no'].isna()\n",
    "    g['supposed_missing'] = g['is_null'] | g['unreliable']\n",
    "\n",
    "    # Reliable subset\n",
    "    reliable = g[~g['supposed_missing']].dropna(subset=['aadhar_no'])\n",
    "\n",
    "    # Unique Aadhaar among reliable\n",
    "    unique_ids = (\n",
    "        reliable.groupby('academic_year')['aadhar_no']\n",
    "                .nunique()\n",
    "                .rename(f'{prefix}_unique_aadhar')\n",
    "    )\n",
    "\n",
    "    # Count of Aadhaar IDs still appearing >1 among reliable rows\n",
    "    if not reliable.empty:\n",
    "        dup_ids = (\n",
    "            reliable.groupby(['academic_year', 'aadhar_no']).size()\n",
    "                    .reset_index(name='cnt')\n",
    "                    .query('cnt > 1')\n",
    "                    .groupby('academic_year').size()\n",
    "                    .rename(f'{prefix}_non_unique_aadhar')\n",
    "        )\n",
    "    else:\n",
    "        dup_ids = pd.Series(dtype='int64', name=f'{prefix}_non_unique_aadhar')\n",
    "\n",
    "    # Supposed-missing row counts\n",
    "    missing_counts = (\n",
    "        g.groupby('academic_year')['supposed_missing']\n",
    "         .sum()\n",
    "         .rename(f'{prefix}_supposed_missing')\n",
    "    )\n",
    "\n",
    "    out = pd.concat([unique_ids, missing_counts, dup_ids], axis=1).reset_index().fillna(0)\n",
    "    for c in out.columns:\n",
    "        if c != 'academic_year':\n",
    "            out[c] = out[c].astype(int)\n",
    "    return out\n",
    "\n",
    "\n",
    "# Build per-year unreliable indices (tune threshold if needed)\n",
    "iti_unrel = build_unreliable_aadhaar_index(\n",
    "    iti_key_df,\n",
    "    per_year_threshold=100,\n",
    "    known_placeholders=KNOWN_AADHAAR_PLACEHOLDERS,\n",
    ")\n",
    "\n",
    "diploma_unrel = build_unreliable_aadhaar_index(\n",
    "    diploma_key_df,\n",
    "    per_year_threshold=100,\n",
    "    known_placeholders=KNOWN_AADHAAR_PLACEHOLDERS,\n",
    ")\n",
    "\n",
    "# Compute clean summaries and merge\n",
    "iti_clean = summarize_clean_aadhaar(iti_key_df, 'iti', iti_unrel)\n",
    "diploma_clean = summarize_clean_aadhaar(diploma_key_df, 'diploma', diploma_unrel)\n",
    "\n",
    "final_clean_summary = (\n",
    "    iti_clean.merge(diploma_clean, on='academic_year', how='outer')\n",
    "             .rename(columns={'academic_year': 'year'})\n",
    "             [[\n",
    "                 'year',\n",
    "                 'iti_unique_aadhar', 'iti_supposed_missing', 'iti_non_unique_aadhar',\n",
    "                 'diploma_unique_aadhar', 'diploma_supposed_missing', 'diploma_non_unique_aadhar',\n",
    "             ]]\n",
    "             .sort_values('year')\n",
    "             .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "final_clean_summary.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b524ba",
   "metadata": {},
   "source": [
    "### 7.6 Final Combined Summary\n",
    "\n",
    "Merged, per academic year, with columns:\n",
    "- Year\n",
    "- ITI and Diploma: applications, unique Aadhaar, 1-by-1 matches\n",
    "\n",
    "1) Applications per year from the application datasets.\n",
    "2) Missing Aadhaar counts from enrollments.\n",
    "3) 1-by-1 Aadhaar  student_key matches.\n",
    "4) Merge everything by academic_year.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb7dad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yearly summary for ITI & Diploma with application counts\n",
    "def create_yearly_summary(enrollment_df, application_df):\n",
    "    \"\"\"\n",
    "    Generate yearly summary combining enrollment and application data.\n",
    "    \n",
    "    Parameters:\n",
    "    - enrollment_df: DataFrame with student_key and aadhar_no (from key generation)\n",
    "    - application_df: DataFrame with barcode for counting total applications\n",
    "    \"\"\"\n",
    "    \n",
    "    # Count total applications by year\n",
    "    app_counts = (\n",
    "        application_df.groupby('academic_year')\n",
    "        .size()\n",
    "        .reset_index(name='total_applications')\n",
    "    )\n",
    "    \n",
    "    # Calculate Aadhaar and 1-by-1 match metrics from enrollment data\n",
    "    def _metrics(g):\n",
    "        mapping = g[['aadhar_no', 'student_key']].drop_duplicates()\n",
    "\n",
    "        # Find unique Aadhaar and unique keys within the year\n",
    "        unique_aadhar = mapping['aadhar_no'].value_counts() == 1\n",
    "        unique_key = mapping['student_key'].value_counts() == 1\n",
    "\n",
    "        # Count 1-by-1 matches (unique Aadhaar to unique key)\n",
    "        one_to_one = mapping[\n",
    "            mapping['aadhar_no'].isin(unique_aadhar[unique_aadhar].index) &\n",
    "            mapping['student_key'].isin(unique_key[unique_key].index)\n",
    "        ]\n",
    "\n",
    "        return pd.Series({\n",
    "            'aadhar': g['aadhar_no'].nunique(),\n",
    "            '1by1_match': len(one_to_one),\n",
    "        })\n",
    "\n",
    "    enroll_summary = (\n",
    "        enrollment_df.groupby('academic_year', group_keys=False)\n",
    "          .apply(_metrics)\n",
    "          .reset_index()\n",
    "    )\n",
    "    \n",
    "    # Merge application counts with enrollment metrics\n",
    "    summary = app_counts.merge(enroll_summary, on='academic_year', how='outer')\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "26a77567",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_9772\\1316644817.py:39: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(_metrics)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ITI Applications: 2,218,985\n",
      "Total Diploma Applications: 1,552,260.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_9772\\1316644817.py:39: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(_metrics)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "year",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "iti_applications",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "iti_aadhar",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "iti_1by1_match",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "diploma_applications",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "diploma_aadhar",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "diploma_1by1_match",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "a93ddc02-b291-434c-83a2-37bef5f2ec6a",
       "rows": [
        [
         "0",
         "2017",
         "145612",
         "27337",
         "26065",
         null,
         null,
         null
        ],
        [
         "1",
         "2018",
         "215662",
         "70613",
         "69617",
         "203293.0",
         "49934.0",
         "48543.0"
        ],
        [
         "2",
         "2019",
         "261505",
         "64148",
         "64119",
         "186285.0",
         "41590.0",
         "41484.0"
        ],
        [
         "3",
         "2020",
         "208581",
         "67412",
         "67407",
         "181710.0",
         "52959.0",
         "52868.0"
        ],
        [
         "4",
         "2021",
         "300121",
         "68000",
         "67974",
         "175738.0",
         "54979.0",
         "54949.0"
        ],
        [
         "5",
         "2022",
         "303082",
         "74104",
         "74077",
         "244165.0",
         "71768.0",
         "71747.0"
        ],
        [
         "6",
         "2023",
         "388380",
         "92085",
         "92069",
         "270782.0",
         "73513.0",
         "73503.0"
        ],
        [
         "7",
         "2024",
         "396042",
         "83958",
         "83958",
         "290287.0",
         "76694.0",
         "76682.0"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>iti_applications</th>\n",
       "      <th>iti_aadhar</th>\n",
       "      <th>iti_1by1_match</th>\n",
       "      <th>diploma_applications</th>\n",
       "      <th>diploma_aadhar</th>\n",
       "      <th>diploma_1by1_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>145612</td>\n",
       "      <td>27337</td>\n",
       "      <td>26065</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>215662</td>\n",
       "      <td>70613</td>\n",
       "      <td>69617</td>\n",
       "      <td>203293.0</td>\n",
       "      <td>49934.0</td>\n",
       "      <td>48543.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019</td>\n",
       "      <td>261505</td>\n",
       "      <td>64148</td>\n",
       "      <td>64119</td>\n",
       "      <td>186285.0</td>\n",
       "      <td>41590.0</td>\n",
       "      <td>41484.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>208581</td>\n",
       "      <td>67412</td>\n",
       "      <td>67407</td>\n",
       "      <td>181710.0</td>\n",
       "      <td>52959.0</td>\n",
       "      <td>52868.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021</td>\n",
       "      <td>300121</td>\n",
       "      <td>68000</td>\n",
       "      <td>67974</td>\n",
       "      <td>175738.0</td>\n",
       "      <td>54979.0</td>\n",
       "      <td>54949.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022</td>\n",
       "      <td>303082</td>\n",
       "      <td>74104</td>\n",
       "      <td>74077</td>\n",
       "      <td>244165.0</td>\n",
       "      <td>71768.0</td>\n",
       "      <td>71747.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023</td>\n",
       "      <td>388380</td>\n",
       "      <td>92085</td>\n",
       "      <td>92069</td>\n",
       "      <td>270782.0</td>\n",
       "      <td>73513.0</td>\n",
       "      <td>73503.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024</td>\n",
       "      <td>396042</td>\n",
       "      <td>83958</td>\n",
       "      <td>83958</td>\n",
       "      <td>290287.0</td>\n",
       "      <td>76694.0</td>\n",
       "      <td>76682.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  iti_applications  iti_aadhar  iti_1by1_match  diploma_applications  \\\n",
       "0  2017            145612       27337           26065                   NaN   \n",
       "1  2018            215662       70613           69617              203293.0   \n",
       "2  2019            261505       64148           64119              186285.0   \n",
       "3  2020            208581       67412           67407              181710.0   \n",
       "4  2021            300121       68000           67974              175738.0   \n",
       "5  2022            303082       74104           74077              244165.0   \n",
       "6  2023            388380       92085           92069              270782.0   \n",
       "7  2024            396042       83958           83958              290287.0   \n",
       "\n",
       "   diploma_aadhar  diploma_1by1_match  \n",
       "0             NaN                 NaN  \n",
       "1         49934.0             48543.0  \n",
       "2         41590.0             41484.0  \n",
       "3         52959.0             52868.0  \n",
       "4         54979.0             54949.0  \n",
       "5         71768.0             71747.0  \n",
       "6         73513.0             73503.0  \n",
       "7         76694.0             76682.0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create summaries for ITI and Diploma\n",
    "iti_summary = create_yearly_summary(iti_key_df, iti_applications)\n",
    "diploma_summary = create_yearly_summary(diploma_key_df, diploma_applications)\n",
    "\n",
    "final_summary = (\n",
    "    iti_summary.merge(\n",
    "        diploma_summary, on='academic_year',how='outer', suffixes=('_iti', '_diploma'))\n",
    "    )\n",
    "\n",
    "# Rename columns for clarity\n",
    "final_summary = final_summary.rename(columns={\n",
    "    'academic_year': 'year',\n",
    "    'total_applications_iti': 'iti_applications',\n",
    "    'aadhar_iti': 'iti_aadhar',\n",
    "    '1by1_match_iti': 'iti_1by1_match',\n",
    "    'total_applications_diploma': 'diploma_applications',\n",
    "    'aadhar_diploma': 'diploma_aadhar',\n",
    "    '1by1_match_diploma': 'diploma_1by1_match',\n",
    "})\n",
    "\n",
    "# Final column ordering\n",
    "final_summary = final_summary[\n",
    "    [\n",
    "        'year',\n",
    "        'iti_applications', 'iti_aadhar', 'iti_1by1_match',\n",
    "        'diploma_applications', 'diploma_aadhar', 'diploma_1by1_match', \n",
    "    ]\n",
    "]\n",
    "\n",
    "# Print total applications across all years\n",
    "print(f\"Total ITI Applications: {final_summary['iti_applications'].sum():,}\")\n",
    "print(f\"Total Diploma Applications: {final_summary['diploma_applications'].sum():,}\\n\")\n",
    "\n",
    "final_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86291a28",
   "metadata": {},
   "source": [
    "#### Step 4  Merge all metrics (by academic_year)\n",
    "\n",
    "Combine ITI and Diploma into one table with:\n",
    "- ITI: applications, unique Aadhaar, 1-by-1 matches\n",
    "- Diploma: applications, unique Aadhaar, 1-by-1 matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d60ac590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITI Summary Table:\n",
      " year  applications  unique_aadhar  supposed_missing  non_unique_aadhar  1by1_match\n",
      " 2017        145612          27336              3181               3631       26065\n",
      " 2018        215662          70612                 1               4242       69617\n",
      " 2019        261505          64148                 0                  0       64119\n",
      " 2020        208581          67411                 4                  0       67407\n",
      " 2021        300121          68000                 0                  0       67974\n",
      " 2022        303082          74104                 0                  0       74077\n",
      " 2023        388380          92085                 0                  0       92069\n",
      " 2024        396042          83958                 0                  0       83958\n",
      "\n",
      "Totals: {'applications': 2218985, 'unique_aadhar': 547654, 'supposed_missing': 3186, 'non_unique_aadhar': 7873, '1by1_match': 545286}\n"
     ]
    }
   ],
   "source": [
    "# Final ITI Summary Table\n",
    "def compute_1by1(df):\n",
    "    p = df[['academic_year', 'aadhar_no', 'student_key']].dropna().drop_duplicates()\n",
    "    a = p.groupby(['academic_year', 'aadhar_no']).size()\n",
    "    k = p.groupby(['academic_year', 'student_key']).size()\n",
    "    m = p.merge(a.rename('a_cnt'), on=['academic_year', 'aadhar_no']).merge(k.rename('k_cnt'), on=['academic_year', 'student_key'])\n",
    "    return m[(m['a_cnt'] == 1) & (m['k_cnt'] == 1)].groupby('academic_year').size().reset_index(name='1by1_match')\n",
    "\n",
    "iti_final = (\n",
    "    iti_applications.groupby('academic_year').size().reset_index(name='applications')\n",
    "    .merge(iti_clean, on='academic_year', how='outer')\n",
    "    .merge(compute_1by1(iti_key_df), on='academic_year', how='outer')\n",
    "    .rename(columns={'academic_year': 'year', 'iti_unique_aadhar': 'unique_aadhar', \n",
    "                     'iti_supposed_missing': 'supposed_missing', 'iti_non_unique_aadhar': 'non_unique_aadhar'})\n",
    "    [['year', 'applications', 'unique_aadhar', 'supposed_missing', 'non_unique_aadhar', '1by1_match']]\n",
    "    .sort_values('year').fillna(0).reset_index(drop=True)\n",
    "    .astype({c: int for c in ['applications', 'unique_aadhar', 'supposed_missing', 'non_unique_aadhar', '1by1_match']})\n",
    ")\n",
    "\n",
    "print(\"ITI Summary Table:\")\n",
    "print(iti_final.to_string(index=False))\n",
    "print(f\"\\nTotals: {iti_final[['applications', 'unique_aadhar', 'supposed_missing', 'non_unique_aadhar', '1by1_match']].sum().to_dict()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d239130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Diploma Summary Table\n",
    "diploma_final = (\n",
    "    diploma_applications.groupby('academic_year').size().reset_index(name='applications')\n",
    "    .merge(diploma_clean, on='academic_year', how='outer')\n",
    "    .merge(compute_1by1(diploma_key_df), on='academic_year', how='outer')\n",
    "    .rename(columns={'academic_year': 'year', 'diploma_unique_aadhar': 'unique_aadhar',\n",
    "                     'diploma_supposed_missing': 'supposed_missing', 'diploma_non_unique_aadhar': 'non_unique_aadhar'})\n",
    "    [['year', 'applications', 'unique_aadhar', 'supposed_missing', 'non_unique_aadhar', '1by1_match']]\n",
    "    .sort_values('year').fillna(0).reset_index(drop=True)\n",
    "    .astype({c: int for c in ['applications', 'unique_aadhar', 'supposed_missing', 'non_unique_aadhar', '1by1_match']})\n",
    ")\n",
    "\n",
    "print(\"Diploma Final Summary Table:\")\n",
    "print(diploma_final.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skills",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
